{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.io import output_notebook\n",
    "import bokeh.plotting as plt\n",
    "from bokeh.models.layouts import LayoutDOM\n",
    "from bokeh.layouts import row, gridplot, Spacer\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import time\n",
    "import scipy.ndimage\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import normalize\n",
    "import matplotlib.cm as cm\n",
    "sys.path.insert(0, '../src')\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebconv.splines import BSplineElement\n",
    "from ebconv.kernel import CardinalBSplineKernel, create_random_centers\n",
    "from ebconv.nn.functional import cbsconv, translate\n",
    "from ebconv.kernel import sampling_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBSConv gradient visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a single basis\n",
    "\n",
    "Let's start by creating a single kernel basis and a centered diract delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (65, 65)\n",
    "KERNEL_SIZE = (10, 10)\n",
    "K = 3\n",
    "SHIFT = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_input_and_kernel(input_size, kernel_size, k):\n",
    "    input_center = np.array(input_size) // 2\n",
    "    kernel_center = np.array(kernel_size) // 2\n",
    "    \n",
    "    input_ = torch.zeros(1, 1, *input_size, dtype=torch.double)\n",
    "    input_[:, :, input_center[0], input_center[1]] = 1\n",
    "    \n",
    "    center = [(0.0, 0.0)]\n",
    "    kernel = CardinalBSplineKernel.create(center, 3, k)\n",
    "    sampling = np.meshgrid(*[sampling_domain(k_s) for k_s in kernel_size], indexing='ij')\n",
    "    basis = kernel(*sampling)[0]\n",
    "\n",
    "    values = []\n",
    "    fig = plt.figure(\n",
    "        match_aspect=True, \n",
    "        x_range=(-input_center[0], input_center[0]), \n",
    "        y_range=(-input_center[1], input_center[1]),\n",
    "        tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")]\n",
    "    )\n",
    "    fig.title.text = 'Input'\n",
    "    fig.image(\n",
    "        image=[input_.data.numpy().squeeze()], \n",
    "        x=-input_center[0], y=-input_center[1],\n",
    "        dw=input_size[0], dh=input_size[1],\n",
    "        palette='Viridis256'\n",
    "    )\n",
    "    values.append(fig)\n",
    "\n",
    "    fig = plt.figure(\n",
    "        match_aspect=True, \n",
    "        x_range=(-input_center[0], input_center[0]), \n",
    "        y_range=(-input_center[1], input_center[1]),\n",
    "        tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")]\n",
    "    )\n",
    "    fig.title.text = 'Kernel'\n",
    "    fig.image(\n",
    "        image=[basis.squeeze()], \n",
    "        x=-kernel_center[0], y=-kernel_center[1],\n",
    "        dw=kernel_size[0], dh=kernel_size[1],\n",
    "        palette='Viridis256'\n",
    "    )\n",
    "    values.append(fig)\n",
    "\n",
    "    grid = gridplot([values])\n",
    "    plt.show(grid)\n",
    "    return input_, kernel, kernel_size\n",
    "    \n",
    "input_, kernel, kernel_size = generate_2d_input_and_kernel(INPUT_SIZE, KERNEL_SIZE, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and shifted convolution\n",
    "Let's now compute the convolution and translate the result. We are doing so to test that during the learning phase we are actually going in the right path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_shift_conv(input_, kernel):\n",
    "    center = torch.tensor(kernel.c, requires_grad=True)\n",
    "    center = center.reshape(1, 1, 2)\n",
    "    scaling = torch.tensor(kernel.s, requires_grad=False).reshape(1, 1)\n",
    "    weights = torch.ones(1, 1, 1, requires_grad=False, dtype=torch.double)\n",
    "\n",
    "    out = cbsconv(input_, KERNEL_SIZE, weights, center, scaling, K)\n",
    "    shifted_out = translate(out.data.clone(), SHIFT)\n",
    "    output_size = np.array(out.shape[2:])\n",
    "    output_center = output_size // 2\n",
    "\n",
    "    values = []\n",
    "    fig = plt.figure(\n",
    "        match_aspect=True, \n",
    "        x_range=(-output_center[0], output_center[0]), \n",
    "        y_range=(-output_center[1], output_center[1]),\n",
    "        tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")]\n",
    "    )\n",
    "    fig.title.text = 'Convolution'\n",
    "    fig.image(\n",
    "        image=[out.data.numpy().squeeze()], \n",
    "        x=-output_center[0], y=-output_center[1],\n",
    "        dw=output_size[0], dh=output_size[1],\n",
    "        palette='Viridis256'\n",
    "    )\n",
    "    values.append(fig)\n",
    "    \n",
    "    fig = plt.figure(\n",
    "        match_aspect=True, \n",
    "        x_range=(-output_center[0], output_center[0]), \n",
    "        y_range=(-output_center[1], output_center[1]),\n",
    "        tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")]\n",
    "    )\n",
    "    fig.title.text = 'Shifted convolution'\n",
    "    fig.image(\n",
    "        image=[shifted_out.data.numpy().squeeze()], \n",
    "        x=-output_center[0], y=-output_center[1],\n",
    "        dw=output_size[0], dh=output_size[1],\n",
    "        palette='Viridis256'\n",
    "    )\n",
    "    values.append(fig)\n",
    "    grid = gridplot([values])\n",
    "    plt.show(grid)\n",
    "    return shifted_out, center, weights, scaling\n",
    "    \n",
    "shifted_conv, center, weights, scaling = conv_shift_conv(input_, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the gradient field\n",
    "\n",
    "Now let's try to compute for each position the gradient to check the gradiant landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient_landscape(input_, target_conv, weights, scaling, n=100):\n",
    "    # Generate a meshgrid to sample the gradient value.\n",
    "    kx = int(KERNEL_SIZE[0] * 1.2)\n",
    "    ky = int(KERNEL_SIZE[1] * 1.2)\n",
    "    x = torch.linspace(0, kx, n, dtype=torch.double)\n",
    "    x -= kx / 2\n",
    "    y = torch.linspace(0, ky, n, dtype=torch.double)\n",
    "    y -= ky / 2\n",
    "    xx, yy = torch.meshgrid(x, y)\n",
    "    xy = torch.stack([xx, yy])\n",
    "    xy = xy.permute(1, 2, 0).reshape(-1, 1, 1, 2)\n",
    "    loss = torch.nn.MSELoss()\n",
    "    gradient = []\n",
    "    loss_data = []\n",
    "    for i, ixy in enumerate(xy):\n",
    "        ixy.requires_grad = True\n",
    "        ixy.retain_grad()\n",
    "        out = cbsconv(input_, KERNEL_SIZE, weights, ixy, scaling, K)\n",
    "        l_out = loss(out, target_conv)\n",
    "        loss_data.append(l_out.item())\n",
    "        if l_out.requires_grad:\n",
    "            l_out.backward()\n",
    "            data = np.concatenate(\n",
    "                [ixy.data.numpy().squeeze(), ixy.grad.data.numpy().squeeze()])\n",
    "            gradient.append(data)\n",
    "            ixy.grad.data.zero_()\n",
    "    return np.array(gradient), (xx.data.numpy(), xy.data.numpy(), loss_data), (kx, ky)\n",
    "\n",
    "gradient, loss_data, samples_size = check_gradient_landscape(input_, shifted_conv, weights, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient(gradient, loss_data, samples_size):\n",
    "    scaling = 1e3\n",
    "    samples_center = np.array(samples_size) / 2\n",
    "    y0 = -gradient[:, 0]\n",
    "    x0 = -gradient[:, 1]\n",
    "    _, _, loss = loss_data\n",
    "    loss = np.flip(np.array(loss)).reshape(100, 100)\n",
    "    x1 = x0 + scaling * gradient[:, 3]\n",
    "    y1 = y0 + scaling * gradient[:, 2]\n",
    "    fig = plt.figure(\n",
    "        x_range = (-samples_center[0], samples_center[0]),\n",
    "        y_range = (-samples_center[1], samples_center[1]),\n",
    "    )\n",
    "    fig.image(\n",
    "        [loss], \n",
    "        x=-samples_center[0], y=-samples_center[1], \n",
    "        dw=samples_size[0], dh=samples_size[1]\n",
    "    )\n",
    "    fig.segment(x0, y0, x1, y1)\n",
    "    fig.title.text = 'Gradient'\n",
    "    \n",
    "    plt.show(fig)\n",
    "    \n",
    "plot_gradient(gradient, loss_data, samples_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
